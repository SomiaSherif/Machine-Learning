#!/usr/bin/env python
# coding: utf-8

# In[3]:


import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
from keras.preprocessing import image
import glob
from PIL import Image
import base64
from PIL import Image
import io


# In[7]:


p = Path("afhq\\train/")
dirs = p.glob("*")
labels_dict = {'cat':0, 'dog':1, 'wild':2}

image_data = []
labels = []    

for folder_dir in dirs:
    label = str(folder_dir).split("\\")[-1]
    
    for img_path in folder_dir.glob("*.jpg"):
        img = image.load_img(img_path, target_size=(32,32))       
        img_array = image.img_to_array(img)
        image_data.append(img_array)
        labels.append(labels_dict[label])


# In[8]:


print(len(image_data))
print(len(labels))


# In[9]:


p = Path("afhq\\val/")
dirs = p.glob("*")
labels_dict = {'cat':0, 'dog':1, 'wild':2}

image_data_val = []
labels_val = []    

for folder_dir in dirs:
    label = str(folder_dir).split("\\")[-1]
    
    for img_path in folder_dir.glob("*.jpg"):
        img = image.load_img(img_path, target_size=(32,32))       
        img_array = image.img_to_array(img)
        image_data_val.append(img_array)
        labels_val.append(labels_dict[label])


# In[10]:


print(len(image_data_val))
print(len(labels_val))


# In[11]:


## Convert data into numpy array

image_data = np.array(image_data, dtype='float32')/255.0
labels = np.array(labels)

print(image_data.shape, labels.shape)


# In[12]:


## Convert validation data into numpy array

image_data_val = np.array(image_data_val, dtype='float32')/255.0
labels_val = np.array(labels_val)

print(image_data_val.shape, labels_val.shape)


# In[13]:


## Randomly shuffle data

import random 
combined = list(zip(image_data, labels))
random.shuffle(combined)

image_data[:], labels[:] = zip(*combined)


# In[14]:


## Randomly shuffle data

import random 
combined = list(zip(image_data_val, labels_val))
random.shuffle(combined)

image_data_val[:], labels_val[:] = zip(*combined)


# In[15]:


## Visualize the data

def drawImg(img):
    import matplotlib.pyplot as plt
    plt.imshow(img)
    plt.axis('off')
    plt.show()
    return

for i in range(10):
    drawImg(image_data[i])


# In[16]:


## Visualize the data

def drawImg(img):
    import matplotlib.pyplot as plt
    plt.imshow(img)
    plt.axis('off')
    plt.show()
    return

for i in range(10):
    drawImg(image_data_val[i])


# In[17]:


# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set 
image_data, X_test, labels, y_test = train_test_split(image_data, labels, test_size=0.2,random_state=109) # 70% training and 30% test


# In[18]:


class SVM:
    def __init__(self,C=1.0):
        self.C = C
        self.W = 0
        self.b = 0
   
    ## Margin

    def hingeLoss(self,W,b,X,Y):
        loss  = 0.0
        
        loss += .5*np.dot(W,W.T)
        
        m = X.shape[0]
        
        for i in range(m):
            ti = Y[i]*(np.dot(W,X[i].T)+b)
            loss += self.C *max(0,(1-ti))
            
        return loss[0][0]
    
    def fit(self,X,Y,batch_size=50,learning_rate=0.001,maxItr=500):
        
        no_of_features = X.shape[1]
        no_of_samples = X.shape[0]
        
        n = learning_rate
        c = self.C
        
        #Init the model parameters
        W = np.zeros((1,no_of_features))
        bias = 0
        
        #Initial Loss
        
        #Training from here...
        # Weight and Bias update rule
        losses = []
        
        for i in range(maxItr):
            #Training Loop
            
            l = self.hingeLoss(W,bias,X,Y)
            losses.append(l)
            ids = np.arange(no_of_samples)
            np.random.shuffle(ids)
            
            #Batch Gradient Descent(Paper) with random shuffling
            for batch_start in range(0,no_of_samples,batch_size):
                #Assume 0 gradient for the batch
                gradw = 0
                gradb = 0
                
                #Iterate over all examples in the mini batch
                for j in range(batch_start,batch_start+batch_size):
                    if j<no_of_samples:
                        i = ids[j]
                        ti =  Y[i]*(np.dot(W,X[i].T)+bias)
                        
                        if ti>1:
                            gradw += 0
                            gradb += 0
                        else:
                            gradw += c*Y[i]*X[i]
                            gradb += c*Y[i]
                     
                #Gradient for the batch is ready! Update W,B
                W = W - n*W + n*gradw
                bias = bias + n*gradb
                
        
        self.W = W
        self.b = bias
        return W,bias,losses


# In[19]:


## Data conversion for One vs One classification

M = image_data.shape[0]
image_data = image_data.reshape(M,-1)
print(image_data.shape)
print(labels.shape)


# In[20]:


## Data conversion for One vs One classification

M = image_data_val.shape[0]
image_data_val = image_data_val.reshape(M,-1)
print(image_data_val.shape)
print(labels_val.shape)


# In[21]:


number_of_classes = len(np.unique(labels))


# In[22]:


number_of_classes_val = len(np.unique(labels_val))


# In[23]:


def classWiseData(x, y):
    data = {}
    
    for i in range(number_of_classes):
        data[i] = []
        
    for i in range(x.shape[0]):
        data[y[i]].append(x[i])
        
    for k in data.keys():
        data[k] = np.array(data[k])
        
    return data


# In[24]:


data = classWiseData(image_data, labels)


# In[25]:


print(data[0].shape[0])
print(data[1].shape[0])
print(data[2].shape[0])


# In[26]:


data_val = classWiseData(image_data_val, labels_val)


# In[27]:


print(data_val[0].shape[0])
print(data_val[1].shape[0])
print(data_val[2].shape[0])


# In[28]:


"""Combines data of two classes into a single matrix"""
def getDataPairForSVM(d1,d2):
    
    l1,l2 = d1.shape[0], d2.shape[0]
    samples = l1+l2
    features = d1.shape[1]
    
    data_pair = np.zeros((samples,features))
    data_labels = np.zeros((samples,))
    
    data_pair[:l1,:] = d1
    data_pair[l1:,:] = d2
    
    data_labels[:l1] = -1
    data_labels[l1:] = 1
    
    return data_pair, data_labels


# In[29]:


mySVM = SVM()
xp, yp = getDataPairForSVM(data[0], data[1])
w,b,loss = mySVM.fit(xp,yp,learning_rate=0.00001,maxItr=100)
plt.plot(loss)


# In[30]:


mySVM = SVM()
xp, yp = getDataPairForSVM(data_val[0], data_val[1])
w,b,loss_val = mySVM.fit(xp,yp,learning_rate=0.00001,maxItr=100)
plt.plot(loss_val)


# In[31]:


def trainSVMs(x,y):
    svm_classifiers = {}
    
    for i in range(number_of_classes):
        svm_classifiers[i] = {}
        for j in range(i+1, number_of_classes):
            xpair,ypair = getDataPairForSVM(data[i],data[j])
            wts,b,loss = mySVM.fit(xpair, ypair,learning_rate=0.00001,maxItr=100)
            svm_classifiers[i][j] = (wts,b)
            
            plt.plot(loss)
            plt.show()
            
    return svm_classifiers


# In[32]:


svm_classifiers = trainSVMs(image_data, labels)


# In[33]:


svm_classifiers_val = trainSVMs(image_data_val, labels_val)


# In[34]:


cats_dogs = svm_classifiers[0][1]
cats_wilds = svm_classifiers[0][2]
print(cats_dogs[0].shape)
print(cats_dogs[1])


# In[35]:


cats_dogs = svm_classifiers_val[0][1]
cats_wilds = svm_classifiers_val[0][2]
print(cats_dogs[0].shape)
print(cats_dogs[1])


# In[36]:


def binaryPredict(x,w,b):
    z = np.dot(x,w.T) + b
    if z >= 0:
        return 1
    else:
        return -1


# In[37]:


def predict(x):
    
    count = np.zeros((number_of_classes,))
    
    for i in range(number_of_classes):
        for j in range(i+1, number_of_classes):
            w,b = svm_classifiers[i][j]
            
            #Take a majority prediction
            z = binaryPredict(x,w,b)
            
            if z==1:
                count[j] += 1
            else:
                count[i] += 1
    
    final_prediction = np.argmax(count)
    return final_prediction


# In[38]:


print(predict(image_data[0]))
print(labels[0])


# In[39]:


print(predict(image_data_val[0]))
print(labels_val[0])


# In[40]:


def accuracy(x,y):
    
    pred = []
    count=0
    
    for i in range(x.shape[0]):
        prediction = predict(x[i])
        pred.append(prediction)
        if prediction==y[i]:
            count += 1
    
    return count/x.shape[0], pred


# In[41]:


acc, ypred = accuracy(image_data, labels)
print(acc*100)


# In[42]:


val_acc, val_ypred = accuracy(image_data_val, labels_val)
print(val_acc*100)


# In[43]:


from sklearn import svm


# In[44]:


svm_classifier = svm.SVC(kernel='linear', C=0.7)
svm_classifier.fit(image_data, labels)
ypred_sklearn = svm_classifier.predict(image_data)
svm_classifier.score(image_data,labels)


# In[45]:


svm_classifier_val = svm.SVC(kernel='linear', C=0.7)
svm_classifier_val.fit(image_data_val, labels_val)
ypred_val_sklearn = svm_classifier_val.predict(image_data_val)
svm_classifier_val.score(image_data_val,labels_val)


# In[46]:


from sklearn.metrics import confusion_matrix


# In[47]:


"""Use this method directly - """
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()


# In[48]:


cnf_matrix = confusion_matrix(labels, ypred)
print(cnf_matrix)


# In[49]:


plot_confusion_matrix(cnf_matrix, [0,1,2],normalize=False,title='Confusion matrix',cmap=plt.cm.Blues)


# In[50]:


cnf_matrix_sklearn = confusion_matrix(labels, ypred_sklearn)
print(cnf_matrix_sklearn)


# In[53]:


plot_confusion_matrix(cnf_matrix_sklearn, [0,1,2],normalize=False,title='Confusion matrix',cmap=plt.cm.Blues)


# In[55]:


from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set 
image_data, X_test, labels, y_test = train_test_split(image_data, labels, test_size=0.2,random_state=109) # 70% training and 30% test

# fit model
clf = OneVsRestClassifier(LogisticRegression())
clf.fit(image_data, labels)
pred = clf.predict(X_test)
pred_prob = clf.predict_proba(X_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 3

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)
    
# plotting    
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('Multiclass ROC',dpi=300); 


# In[ ]:





# In[ ]:




